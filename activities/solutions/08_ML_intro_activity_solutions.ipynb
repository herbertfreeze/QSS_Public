{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised machine learning: Introduction and regularization \n",
    "## Binary classification with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import plotnine\n",
    "from plotnine import *\n",
    "import pickle\n",
    "\n",
    "## nltk imports\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "## sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "## print mult things\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## random\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to process text\n",
    "def processtext(one_str, stop_list):\n",
    "    \n",
    "    ## remove stopwords\n",
    "    no_stop = [tok for tok in wordpunct_tokenize(one_str)\n",
    "              if tok not in stop_list]\n",
    "    \n",
    "    \n",
    "    processed_string = \" \".join([porter.stem(i.lower()) \n",
    "                        for i in no_stop if \n",
    "                        i.lower().isalpha() and len(i) >=3])\n",
    "    return(processed_string)\n",
    "\n",
    "## function to create dtm\n",
    "def create_dtm(list_of_strings, metadata):\n",
    "    vectorizer = CountVectorizer(lowercase = True)\n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names_out())\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(), dtm_dense_named], axis = 1)\n",
    "    return(dtm_dense_named_withid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Load labeled yelp data in `public_data` and run below code\n",
    "\n",
    "**Note**: make sure to change your path if you need to; if you're having trouble loading the `pkl`, try running on jupyter hub since it may be a python versioning issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have trouble loading these data (kernel dies due to memory issues), try sampling down to 5000 or 1000 rows\n",
    "yelp = pd.read_pickle(\"../../public_data/yelp_forML.pkl\")\n",
    "# yelp.head()\n",
    "yelp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess data to create dtm\n",
    "porter = PorterStemmer()\n",
    "list_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "yelp['process_text'] = [processtext(one_review, stop_list = list_stopwords) \n",
    "                        for one_review in yelp['raw_text']]\n",
    "\n",
    "yelp_dtm = create_dtm(yelp['process_text'], yelp[['metadata_label', 'metadata_rowid',\n",
    "                                                 'process_text', 'raw_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yelp_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Split into features, labels, and split into training/hold out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Split into X (features or id metadata) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = yelp_dtm[[col for col in yelp_dtm.columns if col not in ['metadata_label',\n",
    "                                                            'index']]].copy()\n",
    "y = yelp_dtm[['metadata_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 23439)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(15000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking dimensionality\n",
    "X.shape\n",
    "y.shape\n",
    "\n",
    "assert X.shape[0] == y.shape[0]\n",
    "assert y.shape[1] == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 using automatic function to create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using built-in function\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 using more manual approach to create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Population must be a sequence.  For dicts or sets, use sorted(d).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m nrows_test \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m nrows_train\n\u001b[1;32m      6\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m221\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m train_ids \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mset\u001b[39m(X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata_rowid\u001b[39m\u001b[38;5;124m'\u001b[39m]), nrows_train)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmy_split\u001b[39m(X, y, \n\u001b[1;32m     10\u001b[0m              train_ids, \n\u001b[1;32m     11\u001b[0m              id_col):\n\u001b[1;32m     12\u001b[0m     \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m## get test ids\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     test_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(X[id_col])\u001b[38;5;241m.\u001b[39mdifference(train_ids)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/random.py:439\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Sampling without replacement entails tracking either potential\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# selections (the pool) in a list or previous selections in a set.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# too many calls to _randbelow(), making them slower and\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# causing them to eat more entropy than necessary.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(population, _Sequence):\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPopulation must be a sequence.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor dicts or sets, use sorted(d).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    441\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(population)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Population must be a sequence.  For dicts or sets, use sorted(d)."
     ]
    }
   ],
   "source": [
    "### more manually: useful when we want more control\n",
    "### over the ids (eg clustering or time ordering)\n",
    "### or if we want to go back to matrix before preprocessing\n",
    "nrows_train = round(X.shape[0]*0.8)\n",
    "nrows_test = X.shape[0] - nrows_train\n",
    "random.seed(221)\n",
    "train_ids = random.sample(set(X['metadata_rowid']), nrows_train)\n",
    "\n",
    "def my_split(X, y, \n",
    "             train_ids, \n",
    "             id_col):\n",
    "    \n",
    "    ## get test ids\n",
    "    test_ids = set(X[id_col]).difference(train_ids)\n",
    "    \n",
    "    ## split\n",
    "    X_train_man = X[X[id_col].isin(train_ids)].copy()\n",
    "    X_test_man = X[X[id_col].isin(test_ids)].copy()\n",
    "    y_train_man = y[y.index.isin(train_ids)].iloc[:, 0].to_numpy()\n",
    "    y_test_man = y[y.index.isin(test_ids)].iloc[:, 0].to_numpy()\n",
    "    \n",
    "    ## return\n",
    "    return(X_train_man, X_test_man, y_train_man, y_test_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train_man, X_test_man, y_train_man, y_test_man \u001b[38;5;241m=\u001b[39m my_split(X, y, \n\u001b[1;32m      2\u001b[0m                                                             train_ids, \n\u001b[1;32m      3\u001b[0m                                                             id_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata_rowid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_man, X_test_man, y_train_man, y_test_man = my_split(X, y, \n",
    "                                                            train_ids, \n",
    "                                                            id_col = 'metadata_rowid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Estimate models with hardcoded parameters: logistic regression with L1 regularization (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Estimate model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_man' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m non_feat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata_rowid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_text\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocess_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m logit_lasso \u001b[38;5;241m=\u001b[39m LogisticRegression(penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m,max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[1;32m      4\u001b[0m              C \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m logit_lasso\u001b[38;5;241m.\u001b[39mfit(X_train_man[[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m      7\u001b[0m                    non_feat]], y_train_man)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_man' is not defined"
     ]
    }
   ],
   "source": [
    "non_feat = ['metadata_rowid', 'raw_text', 'process_text']\n",
    "\n",
    "logit_lasso = LogisticRegression(penalty = \"l1\",max_iter=100, \n",
    "             C = 0.01, solver='liblinear')\n",
    "\n",
    "logit_lasso.fit(X_train_man[[col for col in X_train.columns if col not in \n",
    "                   non_feat]], y_train_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Generate predictions in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_man' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m logit_lasso\u001b[38;5;241m.\u001b[39mpredict(X_test_man[[col \u001b[38;5;28;01mfor\u001b[39;00m col \n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;129;01min\u001b[39;00m X_test_man\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m non_feat]])\n\u001b[1;32m      3\u001b[0m y_predprob \u001b[38;5;241m=\u001b[39m logit_lasso\u001b[38;5;241m.\u001b[39mpredict_proba(X_test_man[[col \u001b[38;5;28;01mfor\u001b[39;00m col \n\u001b[1;32m      4\u001b[0m                 \u001b[38;5;129;01min\u001b[39;00m X_test_man\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m non_feat]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_man' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = logit_lasso.predict(X_test_man[[col for col \n",
    "                in X_test_man.columns if col not in non_feat]])\n",
    "y_predprob = logit_lasso.predict_proba(X_test_man[[col for col \n",
    "                in X_test_man.columns if col not in non_feat]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the results \n",
    "y_pred[0:10]\n",
    "y_predprob[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Clean up predictions and calculate error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make into a dataframe\n",
    "y_pred_df = pd.DataFrame({'y_pred_binary': y_pred,\n",
    "                         'y_pred_continuous': [one_prob[1] \n",
    "                                            for one_prob in y_predprob],\n",
    "                         'y_true': y_test_man})\n",
    "y_pred_df.sample(n = 10, random_state = 4484)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## precision as tp / tp+fp \n",
    "error_cond = [(y_pred_df['y_true'] == 1) & (y_pred_df['y_pred_binary'] == 1),\n",
    "             (y_pred_df['y_true'] == 1) & (y_pred_df['y_pred_binary'] == 0),\n",
    "              (y_pred_df['y_true'] == 0) & (y_pred_df['y_pred_binary'] == 0)]\n",
    "\n",
    "error_codeto = [\"TP\", \"FN\", \"TN\"]\n",
    "\n",
    "y_pred_df['error_cat'] = np.select(error_cond, error_codeto, default = \"FP\")\n",
    "y_error = y_pred_df.error_cat.value_counts().reset_index()\n",
    "y_error\n",
    "y_error.columns = ['cat', 'n']\n",
    "\n",
    "### precision\n",
    "print(\"Precision is:-----------\")\n",
    "y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0]/(y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0] +\n",
    "                    y_error.loc[y_error.cat == \"FP\", 'n'].iloc[0])\n",
    "\n",
    "### recall\n",
    "print(\"Recall is:---------------\")\n",
    "y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0]/(y_error.loc[y_error.cat == \"TP\", 'n'].iloc[0] +\n",
    "                    y_error.loc[y_error.cat == \"FN\", 'n'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Interpret the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get top features\n",
    "las_coef = pd.DataFrame({'coef': logit_lasso.coef_[0],\n",
    "                         'feature_name': \n",
    "                        [col for col in X_train.columns if col not in non_feat]})\n",
    "las_coef.sort_values(by = 'coef', ascending = False)\n",
    "\n",
    "top_feat = las_coef.sort_values(by = 'coef', ascending = False)[0:10]\n",
    "top_feat_list = top_feat.feature_name.to_list()\n",
    "\n",
    "all_agg = [yelp_dtm.groupby(['metadata_label']).agg({one_feat: np.mean})\n",
    "for one_feat in top_feat_list]\n",
    "all_agg_df = pd.concat(all_agg, axis = 1)\n",
    "all_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compare performance across hyperparameters for logistic regression\n",
    "\n",
    "Would our logit model make more accurate predictions if we fed it different hyperpameters? Which hyperparameters would be the best? Let's find out.\n",
    "\n",
    "1. Define a function that:\n",
    "- takes in a cost parameter (*C*, the inverse of regularization strength)\n",
    "- trains a logistic regression model with L1 regularization (Lasso) and otherwise has the same parameters as above\n",
    "- fits the model on the training data\n",
    "- makes predictions and returns them as a DataFrame\n",
    "\n",
    "2. Use the function to get predictions for the list of *C* parameters below, then bind them into one DataFrame.\n",
    "\n",
    "3. Finally, score the precision for each model (each iteration of *C*) and show which model scores the best.\n",
    "\n",
    "**Hint**: To compute precision score, you can use:\n",
    "```python\n",
    "precision_score(\n",
    "    one_df['y_true'], one_df['y_pred'],\n",
    "    zero_division = 0) # silences warning\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided set of hyperparameters on which to train and then compare performance\n",
    "c_list = np.linspace(4, 0.0001, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution:\n",
    "## define function that takes in one cost parameter\n",
    "## and estimates model, returning pred\n",
    "def one_las(C):\n",
    "    one_lasso = LogisticRegression(penalty = \"l1\", max_iter=100, \n",
    "             C = C, solver='liblinear')\n",
    "    \n",
    "    one_lasso.fit(X_train_man[[col for col in X_train.columns if \n",
    "                              col not in non_feat]], y_train_man)\n",
    "    \n",
    "    y_pred = one_lasso.predict(X_test_man[[col for col in X_test_man.columns \n",
    "                if col not in non_feat]])\n",
    "    \n",
    "    y_pred_df = pd.DataFrame({'y_pred': y_pred, \n",
    "                             'y_true': y_test_man,\n",
    "                             'cost': C})\n",
    "    return(y_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_man' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_pred \u001b[38;5;241m=\u001b[39m [one_las(one_c) \u001b[38;5;28;01mfor\u001b[39;00m one_c \u001b[38;5;129;01min\u001b[39;00m c_list]\n\u001b[1;32m      2\u001b[0m all_pred_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(all_pred)\n\u001b[1;32m      3\u001b[0m all_pred_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_pred \u001b[38;5;241m=\u001b[39m [one_las(one_c) \u001b[38;5;28;01mfor\u001b[39;00m one_c \u001b[38;5;129;01min\u001b[39;00m c_list]\n\u001b[1;32m      2\u001b[0m all_pred_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(all_pred)\n\u001b[1;32m      3\u001b[0m all_pred_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mone_las\u001b[0;34m(C)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_las\u001b[39m(C):\n\u001b[1;32m      5\u001b[0m     one_lasso \u001b[38;5;241m=\u001b[39m LogisticRegression(penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[1;32m      6\u001b[0m              C \u001b[38;5;241m=\u001b[39m C, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     one_lasso\u001b[38;5;241m.\u001b[39mfit(X_train_man[[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \n\u001b[1;32m      9\u001b[0m                               col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m non_feat]], y_train_man)\n\u001b[1;32m     11\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m one_lasso\u001b[38;5;241m.\u001b[39mpredict(X_test_man[[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m X_test_man\u001b[38;5;241m.\u001b[39mcolumns \n\u001b[1;32m     12\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m non_feat]])\n\u001b[1;32m     14\u001b[0m     y_pred_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m'\u001b[39m: y_pred, \n\u001b[1;32m     15\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m'\u001b[39m: y_test_man,\n\u001b[1;32m     16\u001b[0m                              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcost\u001b[39m\u001b[38;5;124m'\u001b[39m: C})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_man' is not defined"
     ]
    }
   ],
   "source": [
    "all_pred = [one_las(one_c) for one_c in c_list]\n",
    "all_pred_df = pd.concat(all_pred)\n",
    "all_pred_df.head()\n",
    "\n",
    "## score one cost level \n",
    "def score_onedf(one_c, all_c):\n",
    "    one_df = all_c[all_c.cost == one_c].copy()\n",
    "    prec_onec =  precision_score(\n",
    "        one_df['y_true'], one_df['y_pred'],\n",
    "        zero_division = 0)\n",
    "    return(prec_onec)\n",
    "    \n",
    "all_score = pd.DataFrame({'cost': c_list,\n",
    "                          'precision': [score_onedf(one_c, all_pred_df) \n",
    "                                  for one_c in c_list]})\n",
    "all_score\n",
    "\n",
    "all_score[all_score.precision == np.max(all_score.precision)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AS = all_score\n",
    "plt.plot(AS.cost.values[:], AS.precision.values[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Activity \n",
    "\n",
    "- Read the documentation here to initialize a ridge regression (l2 penalty)- you can use the same cost parameter (C) and number of iterations as in the lasso example above: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Fit the model on X_train_man, y_train_main \n",
    "- Generate binary and continuous predictions\n",
    "- Create a function that takes in a dataframe of binary predictions and true labels and manually calculates the $F_{1}$ score:\n",
    "\n",
    "$$F_{1} = 2 * \\dfrac{precision * recall}{precision + recall} = \\dfrac{TP}{TP + 0.5(FP + FN)}$$\n",
    "\n",
    "- Apply that function to calculate the F1 score for the decision tree and lasso (from above), and ridge regression (from the activity)\n",
    "- *Challenge exercise*: parametrize the model fitting with a function that takes in a classifier as an argument and returns coefficients or feature importances and certain eval metrics (eg precision, recall, and F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n",
    "logit_ridge = LogisticRegression(penalty = \"l2\",max_iter=100, \n",
    "             C = 1, solver='liblinear')\n",
    "\n",
    "X_feat_tr = X_train_man[[col for col in X_train_man.columns \n",
    "                if col not in non_feat]]\n",
    "X_feat_te = X_test_man[[col for col in X_test_man.columns \n",
    "                if col not in non_feat]]\n",
    "logit_ridge.fit(X_feat_tr, y_train_man)\n",
    "## predict\n",
    "y_hat = logit_ridge.predict(X_feat_te)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_myF1(fit_classifier, X_test, y_test):\n",
    "    \n",
    "    ## predict\n",
    "    y_hat = fit_classifier.predict(X_test)\n",
    "    \n",
    "    ## get the relevant counts\n",
    "    df_pred = pd.DataFrame({'y_true': y_test,\n",
    "                           'y_pred_binary': y_hat})\n",
    "    \n",
    "    ## counts of diff metrics\n",
    "    tp = df_pred[(df_pred.y_true == 1) &\n",
    "            (df_pred.y_pred_binary == 1)].shape[0]\n",
    "    fp = df_pred[(df_pred.y_true == 0) &\n",
    "            (df_pred.y_pred_binary == 1)].shape[0]\n",
    "    fn = df_pred[(df_pred.y_true == 1) &\n",
    "            (df_pred.y_pred_binary == 0)].shape[0]\n",
    "    \n",
    "    ## combine\n",
    "    f1 = (tp)/(tp + 0.5*(fp + fn))\n",
    "    \n",
    "    ## return\n",
    "    return(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_myF1(logit_ridge, X_test = X_feat_te, y_test = y_test_man)\n",
    "calc_myF1(logit_lasso, X_test = X_feat_te, y_test = y_test_man)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_c = pd.DataFrame({'coef': logit_ridge.coef_[0],\n",
    "                        'feat': X_feat_tr.columns})\n",
    "\n",
    "ridge_c[ridge_c.coef != 0].head()\n",
    "ridge_c.sort_values(by = 'coef', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra challenge\n",
    "\n",
    "Text vectorization methods affect downstream classification accuracy. Above, we used simple term counts to turn texts into numbers. This time, instead of using term frequencies, use `sklearn`'s `TfidfVectorizer()` function to weight features with term frequency inverse document frequency (TF-IDF): this gives a word greater weight both when it is more frequent in a text AND when it is rare across the corpus. Does this vectorization approach improve classification accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del yelp_dtm, X, y, all_pred, all_score, logit_ridge, logit_lasso # to conserve memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dtm_tfidf(list_of_strings, metadata):\n",
    "    vectorizer = TfidfVectorizer(lowercase = True)\n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names_out())\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(), dtm_dense_named], axis = 1)\n",
    "    return(dtm_dense_named_withid)\n",
    "\n",
    "## preprocess data to create new dtm with TF-IDF weighting\n",
    "yelp_dtm_tfidf = create_dtm_tfidf(yelp['process_text'], yelp[[\n",
    "    'metadata_label', 'metadata_rowid', 'process_text', 'raw_text']])\n",
    "\n",
    "X_tfidf = yelp_dtm_tfidf[[col for col in yelp_dtm_tfidf.columns if col not in ['metadata_label',\n",
    "                                                            'index']]].copy()\n",
    "y_tfidf = yelp_dtm_tfidf[['metadata_label']]\n",
    "\n",
    "# manual split\n",
    "nrows_train = round(X_tfidf.shape[0]*0.8)\n",
    "nrows_test = X_tfidf.shape[0] - nrows_train\n",
    "random.seed(221)\n",
    "train_ids_tfidf = random.sample(list(set(X_tfidf['metadata_rowid'])), nrows_train)\n",
    "\n",
    "X_train_man_tfidf, X_test_man_tfidf, y_train_man_tfidf, y_test_man_tfidf = my_split(X_tfidf, y_tfidf, \n",
    "                                                                                    train_ids_tfidf, \n",
    "                                                                                   'metadata_rowid')\n",
    "\n",
    "non_feat = ['metadata_rowid', 'raw_text', 'process_text']\n",
    "X_train_man_tfidf = X_train_man_tfidf[[col for col in X_train_man_tfidf.columns \n",
    "                if col not in non_feat]]\n",
    "X_test_man_tfidf = X_test_man_tfidf[[col for col in X_test_man_tfidf.columns \n",
    "                if col not in non_feat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train lasso with L1 reg (lasso), make predictions, get F1 score\n",
    "logit_lasso_tfidf = LogisticRegression(penalty = \"l1\", max_iter=100, \n",
    "             C = 0.01, solver='liblinear')\n",
    "logit_lasso_tfidf.fit(X_train_man_tfidf[[col for col in X_train_man_tfidf.columns if col not in \n",
    "                   non_feat]], y_train_man_tfidf)\n",
    "\n",
    "#y_pred_tfidf = logit_lasso_tfidf.predict(X_test_man_tfidf[[col for col \n",
    "#                in X_test_man_tfidf.columns if col not in non_feat]])\n",
    "#y_predprob_tfidf = logit_lasso_tfidf.predict_proba(X_test_man_tfidf[[col for col \n",
    "#                in X_test_man_tfidf.columns if col not in non_feat]])\n",
    "\n",
    "calc_myF1(logit_lasso_tfidf, X_test = X_test_man_tfidf, y_test = y_test_man_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train lasso with L2 reg (ridge), make predictions, get F1 score\n",
    "logit_ridge_tfidf = LogisticRegression(penalty = \"l2\", max_iter=100, \n",
    "             C = 0.01, solver='liblinear')\n",
    "logit_ridge_tfidf.fit(X_train_man_tfidf[[col for col in X_train_man_tfidf.columns if col not in \n",
    "                   non_feat]], y_train_man_tfidf)\n",
    "\n",
    "#y_pred_tfidf = logit_ridge_tfidf.predict(X_test_man_tfidf[[col for col \n",
    "#                in X_test_man_tfidf.columns if col not in non_feat]])\n",
    "#y_predprob_tfidf = logit_ridge_tfidf.predict_proba(X_test_man_tfidf[[col for col \n",
    "#                in X_test_man_tfidf.columns if col not in non_feat]])\n",
    "\n",
    "calc_myF1(logit_lasso_tfidf, X_test = X_test_man_tfidf, y_test = y_test_man_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
